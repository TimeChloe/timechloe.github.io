---
title: "Multi-Robot Deformable Linear Object Assembly with Multi-Modal Perception"
collection: publications
category: conferences
permalink: /publication/IROS2025
status: submitted
# excerpt: 'This paper is about fixing template issue #693.'
date: 2025-03-02
venue: 'The 2025 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2025) '
# paperurl: 'http://timechloe.github.io/files/2409.11863v1.pdf'
# citation: 'Chen K, Shen Z, Zhang Y, et al. Learning Task Planning from Multi-Modal Demonstration for Multi-Stage Contact-Rich Manipulation[J]. arXiv preprint arXiv:2409.11863, 2024.'
---

<!-- Large Language Models (LLMs) have gained popularity in task planning for long-horizon manipulation tasks. To enhance the validity of LLM-generated plans, visual demonstrations and online videos have been widely employed to guide the planning process. However, for manipulation tasks involving subtle movements but rich contact interactions, visual perception alone may be insufficient for the LLM to fully interpret the demonstration. Additionally, visual data provides limited information on force-related parameters and conditions, which are crucial for effective execution on real robots. In this paper, we introduce an in-context learning framework that incorporates tactile and force-torque information from human demonstrations to enhance LLMs' ability to generate plans for new task scenarios. We propose a bootstrapped reasoning pipeline that sequentially integrates each modality into a comprehensive task plan. This task plan is then used as a reference for planning in new task configurations. Real-world experiments on two different sequential manipulation tasks demonstrate the effectiveness of our framework in improving LLMs' understanding of multi-modal demonstrations and enhancing the overall planning performance. -->